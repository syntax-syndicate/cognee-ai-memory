{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install owlready2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce0f3dbe",
   "metadata": {},
   "source": [
    "# Graphrag Ontology Integration Demo\n",
    "\n",
    "This demonstration shows how graphrag works both with and without ontology integration, highlighting the differences, benefits, and practical applications of using ontological knowledge in a knowledge graph system.\n",
    "\n",
    "## Overview\n",
    "\n",
    "In this demo, we'll:\n",
    "1. Process data without ontology integration\n",
    "2. Process the same data with ontology integration\n",
    "3. Compare search results between the two approaches\n",
    "4. Visualize the differences in knowledge graphs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6259f43",
   "metadata": {},
   "source": [
    "## 1. Setup and Environment\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3f997ca",
   "metadata": {},
   "source": [
    "First, let's set up our environment with the necessary imports:"
   ]
  },
  {
   "cell_type": "code",
   "id": "466e6aa8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:55:11.667149Z",
     "start_time": "2025-02-27T15:55:06.385679Z"
    }
   },
   "source": [
    "import os\n",
    "import asyncio\n",
    "import pathlib\n",
    "from typing import List\n",
    "\n",
    "# Import Cognee utilities\n",
    "from utils import (\n",
    "setup_logging, \n",
    "visualize_graph,\n",
    "get_datasets,\n",
    "get_dataset_data,\n",
    "prune_data,\n",
    "prune_system,\n",
    "add,\n",
    "search,\n",
    "SearchType,\n",
    "get_default_user,\n",
    "KnowledgeGraph,\n",
    "add_data_points\n",
    ")\n",
    "\n",
    "# Import the ontology handling functions\n",
    "from ontology_demo import (\n",
    "owl_testing_pipeline,\n",
    "owl_ontology_merging_layer\n",
    ")\n",
    "\n",
    "import logging\n",
    "setup_logging(logging.INFO)\n",
    "from cognee.tasks.graph import extract_graph_from_data\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "09462fc7",
   "metadata": {},
   "source": [
    "## 2. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9050ae5f",
   "metadata": {},
   "source": [
    "We'll use the same test data for both approaches:"
   ]
  },
  {
   "cell_type": "code",
   "id": "839ec02a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:55:11.673320Z",
     "start_time": "2025-02-27T15:55:11.671108Z"
    }
   },
   "source": [
    "async def prepare_data():\n",
    "    # Clean previous data\n",
    "    await prune_data()\n",
    "    await prune_system(metadata=True)\n",
    "    \n",
    "    # Add test data - the path should point to your data files\n",
    "    current_dir = os.getcwd()\n",
    "    parent_dir = os.path.dirname(current_dir)\n",
    "    file_path = os.path.join(parent_dir, \"ontology_test_input\")\n",
    "    # file_path = os.path.join(\n",
    "    #     os.path.abspath(os.path.join(os.path.dirname(os.path.abspath(__file__)), os.pardir)),\n",
    "    #     \"ontology_test_input\"\n",
    "    # )\n",
    "    await add(file_path)\n",
    "    \n",
    "    print(\"Data prepared successfully\")\n"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "markdown",
   "id": "5ed7d228",
   "metadata": {},
   "source": [
    "## 3. Standard Knowledge Graph Processing (Without Ontology)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b865bcdb",
   "metadata": {},
   "source": [
    "Let's process our data using the standard graphrag pipeline without ontology integration:"
   ]
  },
  {
   "cell_type": "code",
   "id": "075f3bb1",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:55:11.716549Z",
     "start_time": "2025-02-27T15:55:11.713162Z"
    }
   },
   "source": [
    "async def process_without_ontology():\n",
    "    # Get the dataset to process\n",
    "    user = await get_default_user()\n",
    "    datasets = await get_datasets(user.id)\n",
    "    \n",
    "    if not datasets:\n",
    "        print(\"No datasets found!\")\n",
    "        return\n",
    "    \n",
    "    # Use the standard pipeline\n",
    "    from utils import (\n",
    "        run_tasks,\n",
    "        Task,\n",
    "        classify_documents,\n",
    "        check_permissions_on_documents,\n",
    "        extract_chunks_from_documents,\n",
    "        extract_content_graph,\n",
    "        get_max_chunk_tokens\n",
    "    )\n",
    "    \n",
    "    for dataset in datasets:\n",
    "        data_documents = await get_dataset_data(dataset_id=dataset.id)\n",
    "\n",
    "        \n",
    "        tasks = [\n",
    "            Task(classify_documents),\n",
    "            Task(check_permissions_on_documents, user=user, permissions=[\"write\"]),\n",
    "            Task(extract_chunks_from_documents, max_chunk_tokens=get_max_chunk_tokens()),\n",
    "            Task(\n",
    "                extract_graph_from_data, graph_model=KnowledgeGraph, task_config={\"batch_size\": 10}\n",
    "            ),  # Generate knowledge graphs from the document chunks.\n",
    "            Task(add_data_points, task_config={\"batch_size\": 10}),\n",
    "        ]\n",
    "        \n",
    "        \n",
    "        pipeline_run = run_tasks(tasks, dataset.id, data_documents, \"standard_pipeline\")\n",
    "        \n",
    "        async for run_status in pipeline_run:\n",
    "            print(run_status)\n",
    "    \n",
    "    # Save graph visualization\n",
    "    notebook_dir = pathlib.Path.cwd()\n",
    "    output_dir = notebook_dir / \".artifacts\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    standard_graph_path = (output_dir / \"standard_graph_visualization.html\").resolve()\n",
    "    await visualize_graph(str(standard_graph_path))\n",
    "    \n",
    "    print(f\"Standard graph saved to: {standard_graph_path}\")\n",
    "    return standard_graph_path"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "fe93b9c5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:55:11.741081Z",
     "start_time": "2025-02-27T15:55:11.724588Z"
    }
   },
   "source": [
    "import pathlib\n",
    "import os\n",
    "from utils  import visualize_graph\n",
    "\n",
    "# Use the current working directory instead of __file__:\n",
    "notebook_dir = pathlib.Path.cwd()\n",
    "\n",
    "graph_file_path = (notebook_dir / \".artifacts\" / \"graph_visualization.html\").resolve()\n",
    "\n",
    "# Make sure to convert to string if visualize_graph expects a string\n",
    "b = await visualize_graph(str(graph_file_path))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-27 07:55:11,739 - INFO - Graph visualization saved as /Users/vasilije/cognee/cognee/ontology_testing_SANDBOX/Ontology_demo/.artifacts/graph_visualization.html\n",
      "2025-02-27 07:55:11,739 - INFO - The HTML file has been stored at path: /Users/vasilije/cognee/cognee/ontology_testing_SANDBOX/Ontology_demo/.artifacts/graph_visualization.html\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "ef009ee7",
   "metadata": {},
   "source": [
    "## 4. Ontology-Enhanced Processing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0234f4f4",
   "metadata": {},
   "source": [
    "\n",
    "Now, let's process the same data with ontology integration:"
   ]
  },
  {
   "cell_type": "code",
   "id": "af291c5d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:55:11.751417Z",
     "start_time": "2025-02-27T15:55:11.749056Z"
    }
   },
   "source": [
    "\n",
    "async def process_with_ontology():\n",
    "    # This uses the owl_testing_pipeline from ontology_demo.py\n",
    "    await owl_testing_pipeline()\n",
    "    \n",
    "    # Save graph visualization\n",
    "    notebook_dir = pathlib.Path.cwd()\n",
    "    output_dir = notebook_dir / \".artifacts\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    ontology_graph_path = (output_dir / \"ontology_graph_visualization.html\").resolve()\n",
    "    await visualize_graph(str(ontology_graph_path))\n",
    "    \n",
    "    print(f\"Ontology-enhanced graph saved to: {ontology_graph_path}\")\n",
    "    return ontology_graph_path"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "44ae5800",
   "metadata": {},
   "source": [
    "## 5. Comparing Search Results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3878d623",
   "metadata": {},
   "source": [
    "\n",
    "Let's execute some queries to compare the results:"
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-27T15:55:30.134164Z",
     "start_time": "2025-02-27T15:55:13.892748Z"
    }
   },
   "cell_type": "code",
   "source": [
    "queries = [\n",
    "    \"What are the exact cars produced by Audi and what are their types?\",\n",
    "    \"What features do luxury cars have?\",\n",
    "    \"Tell me about vehicle manufacturers and their relationships\"\n",
    "]\n",
    "\n",
    "print(\"==== STANDARD KNOWLEDGE GRAPH SEARCH RESULTS ====\")\n",
    "# First, search using the standard graph\n",
    "await prune_data()  \n",
    "await process_without_ontology()\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    results = await search(query_type=SearchType.GRAPH_COMPLETION, query_text=query)\n",
    "    print(\"Results:\")\n",
    "    for i, result in enumerate(results[:3]):\n",
    "        print(f\"{i+1}. {result}\")"
   ],
   "id": "e5316a01c80d649e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==== STANDARD KNOWLEDGE GRAPH SEARCH RESULTS ====\n",
      "<cognee.modules.pipelines.models.PipelineRun.PipelineRun object at 0x34472a950>\n",
      "2025-02-27 07:55:14,123 - INFO - Pipeline run started: `ea9bd1d8-7bd9-5908-a88b-1192a13ed265`\n",
      "2025-02-27 07:55:14,123 - INFO - Coroutine task started: `classify_documents`\n",
      "2025-02-27 07:55:14,123 - INFO - Coroutine task started: `check_permissions_on_documents`\n",
      "2025-02-27 07:55:14,130 - INFO - Async generator task started: `extract_chunks_from_documents`\n",
      "2025-02-27 07:55:14,137 - INFO - Coroutine task started: `extract_graph_from_data`\n",
      "2025-02-27 07:55:14,140 - WARNING - Langfuse client is disabled since no public_key was provided as a parameter or environment variable 'LANGFUSE_PUBLIC_KEY'. See our docs: https://langfuse.com/docs/sdk/python/low-level-sdk#initialize-client\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[92m07:55:14 - LiteLLM:INFO\u001B[0m: utils.py:2784 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-27 07:55:14,156 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[92m07:55:14 - LiteLLM:INFO\u001B[0m: utils.py:2784 - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-27 07:55:14,160 - INFO - \n",
      "LiteLLM completion() model= gpt-4o-mini; provider = openai\n",
      "2025-02-27 07:55:29,678 - ERROR - Coroutine task errored: `extract_graph_from_data`\n",
      "Length of item not correct: expected 768 but got array of size 3072\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 155, in run_tasks_base\n",
      "    task_result = await running_task.run(*args)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/graph/extract_graph_from_data.py\", line 58, in extract_graph_from_data\n",
      "    return await integrate_chunk_graphs(data_chunks, chunk_graphs, graph_model)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/graph/extract_graph_from_data.py\", line 43, in integrate_chunk_graphs\n",
      "    await add_data_points(graph_nodes)\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/storage/add_data_points.py\", line 37, in add_data_points\n",
      "    await index_data_points(nodes)\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/storage/index_data_points.py\", line 41, in index_data_points\n",
      "    await vector_engine.index_data_points(index_name, field_name, indexable_points)\n",
      "  File \"/Users/vasilije/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py\", line 250, in index_data_points\n",
      "    await self.create_data_points(\n",
      "  File \"/Users/vasilije/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py\", line 118, in create_data_points\n",
      "    await (\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 2765, in _do_merge\n",
      "    data, _ = _sanitize_data(\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 164, in _sanitize_data\n",
      "    data = _coerce_to_table(data, schema)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 117, in _coerce_to_table\n",
      "    return pa.Table.from_pylist(data, schema=schema)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/table.pxi\", line 2032, in pyarrow.lib._Tabular.from_pylist\n",
      "  File \"pyarrow/table.pxi\", line 6396, in pyarrow.lib._from_pylist\n",
      "  File \"pyarrow/table.pxi\", line 4851, in pyarrow.lib.Table.from_arrays\n",
      "  File \"pyarrow/table.pxi\", line 1608, in pyarrow.lib._sanitize_arrays\n",
      "  File \"pyarrow/array.pxi\", line 402, in pyarrow.lib.asarray\n",
      "  File \"pyarrow/array.pxi\", line 372, in pyarrow.lib.array\n",
      "  File \"pyarrow/array.pxi\", line 42, in pyarrow.lib._sequence_to_array\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: Length of item not correct: expected 768 but got array of size 30722025-02-27 07:55:29,683 - ERROR - Async generator task errored: `extract_chunks_from_documents`\n",
      "Length of item not correct: expected 768 but got array of size 3072\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 63, in run_tasks_base\n",
      "    async for result in run_tasks_base(leftover_tasks, results, user):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 182, in run_tasks_base\n",
      "    raise error\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 155, in run_tasks_base\n",
      "    task_result = await running_task.run(*args)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/graph/extract_graph_from_data.py\", line 58, in extract_graph_from_data\n",
      "    return await integrate_chunk_graphs(data_chunks, chunk_graphs, graph_model)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/graph/extract_graph_from_data.py\", line 43, in integrate_chunk_graphs\n",
      "    await add_data_points(graph_nodes)\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/storage/add_data_points.py\", line 37, in add_data_points\n",
      "    await index_data_points(nodes)\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/storage/index_data_points.py\", line 41, in index_data_points\n",
      "    await vector_engine.index_data_points(index_name, field_name, indexable_points)\n",
      "  File \"/Users/vasilije/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py\", line 250, in index_data_points\n",
      "    await self.create_data_points(\n",
      "  File \"/Users/vasilije/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py\", line 118, in create_data_points\n",
      "    await (\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 2765, in _do_merge\n",
      "    data, _ = _sanitize_data(\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 164, in _sanitize_data\n",
      "    data = _coerce_to_table(data, schema)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 117, in _coerce_to_table\n",
      "    return pa.Table.from_pylist(data, schema=schema)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/table.pxi\", line 2032, in pyarrow.lib._Tabular.from_pylist\n",
      "  File \"pyarrow/table.pxi\", line 6396, in pyarrow.lib._from_pylist\n",
      "  File \"pyarrow/table.pxi\", line 4851, in pyarrow.lib.Table.from_arrays\n",
      "  File \"pyarrow/table.pxi\", line 1608, in pyarrow.lib._sanitize_arrays\n",
      "  File \"pyarrow/array.pxi\", line 402, in pyarrow.lib.asarray\n",
      "  File \"pyarrow/array.pxi\", line 372, in pyarrow.lib.array\n",
      "  File \"pyarrow/array.pxi\", line 42, in pyarrow.lib._sequence_to_array\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: Length of item not correct: expected 768 but got array of size 30722025-02-27 07:55:29,684 - ERROR - Coroutine task errored: `check_permissions_on_documents`\n",
      "Length of item not correct: expected 768 but got array of size 3072\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 157, in run_tasks_base\n",
      "    async for result in run_tasks_base(leftover_tasks, task_result, user):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 90, in run_tasks_base\n",
      "    raise error\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 63, in run_tasks_base\n",
      "    async for result in run_tasks_base(leftover_tasks, results, user):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 182, in run_tasks_base\n",
      "    raise error\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 155, in run_tasks_base\n",
      "    task_result = await running_task.run(*args)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/graph/extract_graph_from_data.py\", line 58, in extract_graph_from_data\n",
      "    return await integrate_chunk_graphs(data_chunks, chunk_graphs, graph_model)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/graph/extract_graph_from_data.py\", line 43, in integrate_chunk_graphs\n",
      "    await add_data_points(graph_nodes)\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/storage/add_data_points.py\", line 37, in add_data_points\n",
      "    await index_data_points(nodes)\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/storage/index_data_points.py\", line 41, in index_data_points\n",
      "    await vector_engine.index_data_points(index_name, field_name, indexable_points)\n",
      "  File \"/Users/vasilije/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py\", line 250, in index_data_points\n",
      "    await self.create_data_points(\n",
      "  File \"/Users/vasilije/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py\", line 118, in create_data_points\n",
      "    await (\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 2765, in _do_merge\n",
      "    data, _ = _sanitize_data(\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 164, in _sanitize_data\n",
      "    data = _coerce_to_table(data, schema)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 117, in _coerce_to_table\n",
      "    return pa.Table.from_pylist(data, schema=schema)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/table.pxi\", line 2032, in pyarrow.lib._Tabular.from_pylist\n",
      "  File \"pyarrow/table.pxi\", line 6396, in pyarrow.lib._from_pylist\n",
      "  File \"pyarrow/table.pxi\", line 4851, in pyarrow.lib.Table.from_arrays\n",
      "  File \"pyarrow/table.pxi\", line 1608, in pyarrow.lib._sanitize_arrays\n",
      "  File \"pyarrow/array.pxi\", line 402, in pyarrow.lib.asarray\n",
      "  File \"pyarrow/array.pxi\", line 372, in pyarrow.lib.array\n",
      "  File \"pyarrow/array.pxi\", line 42, in pyarrow.lib._sequence_to_array\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: Length of item not correct: expected 768 but got array of size 30722025-02-27 07:55:29,685 - ERROR - Coroutine task errored: `classify_documents`\n",
      "Length of item not correct: expected 768 but got array of size 3072\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 157, in run_tasks_base\n",
      "    async for result in run_tasks_base(leftover_tasks, task_result, user):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 182, in run_tasks_base\n",
      "    raise error\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 157, in run_tasks_base\n",
      "    async for result in run_tasks_base(leftover_tasks, task_result, user):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 90, in run_tasks_base\n",
      "    raise error\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 63, in run_tasks_base\n",
      "    async for result in run_tasks_base(leftover_tasks, results, user):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 182, in run_tasks_base\n",
      "    raise error\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 155, in run_tasks_base\n",
      "    task_result = await running_task.run(*args)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/graph/extract_graph_from_data.py\", line 58, in extract_graph_from_data\n",
      "    return await integrate_chunk_graphs(data_chunks, chunk_graphs, graph_model)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/graph/extract_graph_from_data.py\", line 43, in integrate_chunk_graphs\n",
      "    await add_data_points(graph_nodes)\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/storage/add_data_points.py\", line 37, in add_data_points\n",
      "    await index_data_points(nodes)\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/storage/index_data_points.py\", line 41, in index_data_points\n",
      "    await vector_engine.index_data_points(index_name, field_name, indexable_points)\n",
      "  File \"/Users/vasilije/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py\", line 250, in index_data_points\n",
      "    await self.create_data_points(\n",
      "  File \"/Users/vasilije/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py\", line 118, in create_data_points\n",
      "    await (\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 2765, in _do_merge\n",
      "    data, _ = _sanitize_data(\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 164, in _sanitize_data\n",
      "    data = _coerce_to_table(data, schema)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 117, in _coerce_to_table\n",
      "    return pa.Table.from_pylist(data, schema=schema)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/table.pxi\", line 2032, in pyarrow.lib._Tabular.from_pylist\n",
      "  File \"pyarrow/table.pxi\", line 6396, in pyarrow.lib._from_pylist\n",
      "  File \"pyarrow/table.pxi\", line 4851, in pyarrow.lib.Table.from_arrays\n",
      "  File \"pyarrow/table.pxi\", line 1608, in pyarrow.lib._sanitize_arrays\n",
      "  File \"pyarrow/array.pxi\", line 402, in pyarrow.lib.asarray\n",
      "  File \"pyarrow/array.pxi\", line 372, in pyarrow.lib.array\n",
      "  File \"pyarrow/array.pxi\", line 42, in pyarrow.lib._sequence_to_array\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: Length of item not correct: expected 768 but got array of size 30722025-02-27 07:55:29,687 - ERROR - Pipeline run errored: `ea9bd1d8-7bd9-5908-a88b-1192a13ed265`\n",
      "Length of item not correct: expected 768 but got array of size 3072\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 242, in run_tasks_with_telemetry\n",
      "    async for result in run_tasks_base(tasks, data, user):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 182, in run_tasks_base\n",
      "    raise error\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 157, in run_tasks_base\n",
      "    async for result in run_tasks_base(leftover_tasks, task_result, user):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 182, in run_tasks_base\n",
      "    raise error\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 157, in run_tasks_base\n",
      "    async for result in run_tasks_base(leftover_tasks, task_result, user):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 90, in run_tasks_base\n",
      "    raise error\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 63, in run_tasks_base\n",
      "    async for result in run_tasks_base(leftover_tasks, results, user):\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 182, in run_tasks_base\n",
      "    raise error\n",
      "  File \"/Users/vasilije/cognee/cognee/modules/pipelines/operations/run_tasks.py\", line 155, in run_tasks_base\n",
      "    task_result = await running_task.run(*args)\n",
      "                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/graph/extract_graph_from_data.py\", line 58, in extract_graph_from_data\n",
      "    return await integrate_chunk_graphs(data_chunks, chunk_graphs, graph_model)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/graph/extract_graph_from_data.py\", line 43, in integrate_chunk_graphs\n",
      "    await add_data_points(graph_nodes)\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/storage/add_data_points.py\", line 37, in add_data_points\n",
      "    await index_data_points(nodes)\n",
      "  File \"/Users/vasilije/cognee/cognee/tasks/storage/index_data_points.py\", line 41, in index_data_points\n",
      "    await vector_engine.index_data_points(index_name, field_name, indexable_points)\n",
      "  File \"/Users/vasilije/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py\", line 250, in index_data_points\n",
      "    await self.create_data_points(\n",
      "  File \"/Users/vasilije/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py\", line 118, in create_data_points\n",
      "    await (\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 2765, in _do_merge\n",
      "    data, _ = _sanitize_data(\n",
      "              ^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 164, in _sanitize_data\n",
      "    data = _coerce_to_table(data, schema)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py\", line 117, in _coerce_to_table\n",
      "    return pa.Table.from_pylist(data, schema=schema)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"pyarrow/table.pxi\", line 2032, in pyarrow.lib._Tabular.from_pylist\n",
      "  File \"pyarrow/table.pxi\", line 6396, in pyarrow.lib._from_pylist\n",
      "  File \"pyarrow/table.pxi\", line 4851, in pyarrow.lib.Table.from_arrays\n",
      "  File \"pyarrow/table.pxi\", line 1608, in pyarrow.lib._sanitize_arrays\n",
      "  File \"pyarrow/array.pxi\", line 402, in pyarrow.lib.asarray\n",
      "  File \"pyarrow/array.pxi\", line 372, in pyarrow.lib.array\n",
      "  File \"pyarrow/array.pxi\", line 42, in pyarrow.lib._sequence_to_array\n",
      "  File \"pyarrow/error.pxi\", line 155, in pyarrow.lib.pyarrow_internal_check_status\n",
      "  File \"pyarrow/error.pxi\", line 92, in pyarrow.lib.check_status\n",
      "pyarrow.lib.ArrowInvalid: Length of item not correct: expected 768 but got array of size 3072<cognee.modules.pipelines.models.PipelineRun.PipelineRun object at 0x3467f38d0>\n"
     ]
    },
    {
     "ename": "ArrowInvalid",
     "evalue": "Length of item not correct: expected 768 but got array of size 3072",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mArrowInvalid\u001B[0m                              Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[6], line 10\u001B[0m\n\u001B[1;32m      8\u001B[0m \u001B[38;5;66;03m# First, search using the standard graph\u001B[39;00m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m prune_data()  \n\u001B[0;32m---> 10\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m process_without_ontology()\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m query \u001B[38;5;129;01min\u001B[39;00m queries:\n\u001B[1;32m     13\u001B[0m     \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mQuery: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mquery\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[3], line 38\u001B[0m, in \u001B[0;36mprocess_without_ontology\u001B[0;34m()\u001B[0m\n\u001B[1;32m     25\u001B[0m     tasks \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m     26\u001B[0m         Task(classify_documents),\n\u001B[1;32m     27\u001B[0m         Task(check_permissions_on_documents, user\u001B[38;5;241m=\u001B[39muser, permissions\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mwrite\u001B[39m\u001B[38;5;124m\"\u001B[39m]),\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     32\u001B[0m         Task(add_data_points, task_config\u001B[38;5;241m=\u001B[39m{\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mbatch_size\u001B[39m\u001B[38;5;124m\"\u001B[39m: \u001B[38;5;241m10\u001B[39m}),\n\u001B[1;32m     33\u001B[0m     ]\n\u001B[1;32m     36\u001B[0m     pipeline_run \u001B[38;5;241m=\u001B[39m run_tasks(tasks, dataset\u001B[38;5;241m.\u001B[39mid, data_documents, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mstandard_pipeline\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 38\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m run_status \u001B[38;5;129;01min\u001B[39;00m pipeline_run:\n\u001B[1;32m     39\u001B[0m         \u001B[38;5;28mprint\u001B[39m(run_status)\n\u001B[1;32m     41\u001B[0m \u001B[38;5;66;03m# Save graph visualization\u001B[39;00m\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:293\u001B[0m, in \u001B[0;36mrun_tasks\u001B[0;34m(tasks, dataset_id, data, pipeline_name)\u001B[0m\n\u001B[1;32m    291\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m \u001B[38;5;167;01mException\u001B[39;00m \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m    292\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m log_pipeline_run_error(pipeline_run_id, pipeline_id, dataset_id, data, e)\n\u001B[0;32m--> 293\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m e\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:286\u001B[0m, in \u001B[0;36mrun_tasks\u001B[0;34m(tasks, dataset_id, data, pipeline_name)\u001B[0m\n\u001B[1;32m    283\u001B[0m pipeline_run_id \u001B[38;5;241m=\u001B[39m pipeline_run\u001B[38;5;241m.\u001B[39mpipeline_run_id\n\u001B[1;32m    285\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 286\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m _ \u001B[38;5;129;01min\u001B[39;00m run_tasks_with_telemetry(tasks, data, pipeline_id):\n\u001B[1;32m    287\u001B[0m         \u001B[38;5;28;01mpass\u001B[39;00m\n\u001B[1;32m    289\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m log_pipeline_run_complete(pipeline_run_id, pipeline_id, dataset_id, data)\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:269\u001B[0m, in \u001B[0;36mrun_tasks_with_telemetry\u001B[0;34m(tasks, data, pipeline_name)\u001B[0m\n\u001B[1;32m    254\u001B[0m logger\u001B[38;5;241m.\u001B[39merror(\n\u001B[1;32m    255\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline run errored: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    256\u001B[0m     pipeline_name,\n\u001B[1;32m    257\u001B[0m     \u001B[38;5;28mstr\u001B[39m(error),\n\u001B[1;32m    258\u001B[0m     exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    259\u001B[0m )\n\u001B[1;32m    260\u001B[0m send_telemetry(\n\u001B[1;32m    261\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline Run Errored\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    262\u001B[0m     user\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    266\u001B[0m     \u001B[38;5;241m|\u001B[39m config,\n\u001B[1;32m    267\u001B[0m )\n\u001B[0;32m--> 269\u001B[0m \u001B[38;5;28;01mraise\u001B[39;00m error\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:242\u001B[0m, in \u001B[0;36mrun_tasks_with_telemetry\u001B[0;34m(tasks, data, pipeline_name)\u001B[0m\n\u001B[1;32m    232\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline run started: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m, pipeline_name)\n\u001B[1;32m    233\u001B[0m send_telemetry(\n\u001B[1;32m    234\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline Run Started\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    235\u001B[0m     user\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    239\u001B[0m     \u001B[38;5;241m|\u001B[39m config,\n\u001B[1;32m    240\u001B[0m )\n\u001B[0;32m--> 242\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m run_tasks_base(tasks, data, user):\n\u001B[1;32m    243\u001B[0m     \u001B[38;5;28;01myield\u001B[39;00m result\n\u001B[1;32m    245\u001B[0m logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mPipeline run completed: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m, pipeline_name)\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:182\u001B[0m, in \u001B[0;36mrun_tasks_base\u001B[0;34m(tasks, data, user)\u001B[0m\n\u001B[1;32m    169\u001B[0m         logger\u001B[38;5;241m.\u001B[39merror(\n\u001B[1;32m    170\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCoroutine task errored: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    171\u001B[0m             running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    172\u001B[0m             \u001B[38;5;28mstr\u001B[39m(error),\n\u001B[1;32m    173\u001B[0m             exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    174\u001B[0m         )\n\u001B[1;32m    175\u001B[0m         send_telemetry(\n\u001B[1;32m    176\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCoroutine Task Errored\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    177\u001B[0m             user\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    180\u001B[0m             },\n\u001B[1;32m    181\u001B[0m         )\n\u001B[0;32m--> 182\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misfunction(running_task\u001B[38;5;241m.\u001B[39mexecutable):\n\u001B[1;32m    185\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFunction task started: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m, running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:157\u001B[0m, in \u001B[0;36mrun_tasks_base\u001B[0;34m(tasks, data, user)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m     task_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m running_task\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m--> 157\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m run_tasks_base(leftover_tasks, task_result, user):\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m result\n\u001B[1;32m    160\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCoroutine task completed: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m, running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:182\u001B[0m, in \u001B[0;36mrun_tasks_base\u001B[0;34m(tasks, data, user)\u001B[0m\n\u001B[1;32m    169\u001B[0m         logger\u001B[38;5;241m.\u001B[39merror(\n\u001B[1;32m    170\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCoroutine task errored: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    171\u001B[0m             running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    172\u001B[0m             \u001B[38;5;28mstr\u001B[39m(error),\n\u001B[1;32m    173\u001B[0m             exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    174\u001B[0m         )\n\u001B[1;32m    175\u001B[0m         send_telemetry(\n\u001B[1;32m    176\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCoroutine Task Errored\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    177\u001B[0m             user\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    180\u001B[0m             },\n\u001B[1;32m    181\u001B[0m         )\n\u001B[0;32m--> 182\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misfunction(running_task\u001B[38;5;241m.\u001B[39mexecutable):\n\u001B[1;32m    185\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFunction task started: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m, running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:157\u001B[0m, in \u001B[0;36mrun_tasks_base\u001B[0;34m(tasks, data, user)\u001B[0m\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[1;32m    155\u001B[0m     task_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m running_task\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;241m*\u001B[39margs)\n\u001B[0;32m--> 157\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m run_tasks_base(leftover_tasks, task_result, user):\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m result\n\u001B[1;32m    160\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCoroutine task completed: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m, running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:90\u001B[0m, in \u001B[0;36mrun_tasks_base\u001B[0;34m(tasks, data, user)\u001B[0m\n\u001B[1;32m     77\u001B[0m         logger\u001B[38;5;241m.\u001B[39merror(\n\u001B[1;32m     78\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsync generator task errored: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     79\u001B[0m             running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m     80\u001B[0m             \u001B[38;5;28mstr\u001B[39m(error),\n\u001B[1;32m     81\u001B[0m             exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m     82\u001B[0m         )\n\u001B[1;32m     83\u001B[0m         send_telemetry(\n\u001B[1;32m     84\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mAsync Generator Task Errored\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m     85\u001B[0m             user\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m     88\u001B[0m             },\n\u001B[1;32m     89\u001B[0m         )\n\u001B[0;32m---> 90\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m     92\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misgeneratorfunction(running_task\u001B[38;5;241m.\u001B[39mexecutable):\n\u001B[1;32m     93\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mGenerator task started: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m, running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:63\u001B[0m, in \u001B[0;36mrun_tasks_base\u001B[0;34m(tasks, data, user)\u001B[0m\n\u001B[1;32m     60\u001B[0m         results \u001B[38;5;241m=\u001B[39m []\n\u001B[1;32m     62\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(results) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 63\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m run_tasks_base(leftover_tasks, results, user):\n\u001B[1;32m     64\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m result\n\u001B[1;32m     66\u001B[0m     results \u001B[38;5;241m=\u001B[39m []\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:182\u001B[0m, in \u001B[0;36mrun_tasks_base\u001B[0;34m(tasks, data, user)\u001B[0m\n\u001B[1;32m    169\u001B[0m         logger\u001B[38;5;241m.\u001B[39merror(\n\u001B[1;32m    170\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCoroutine task errored: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    171\u001B[0m             running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m,\n\u001B[1;32m    172\u001B[0m             \u001B[38;5;28mstr\u001B[39m(error),\n\u001B[1;32m    173\u001B[0m             exc_info\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m,\n\u001B[1;32m    174\u001B[0m         )\n\u001B[1;32m    175\u001B[0m         send_telemetry(\n\u001B[1;32m    176\u001B[0m             \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCoroutine Task Errored\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    177\u001B[0m             user\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    180\u001B[0m             },\n\u001B[1;32m    181\u001B[0m         )\n\u001B[0;32m--> 182\u001B[0m         \u001B[38;5;28;01mraise\u001B[39;00m error\n\u001B[1;32m    184\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m inspect\u001B[38;5;241m.\u001B[39misfunction(running_task\u001B[38;5;241m.\u001B[39mexecutable):\n\u001B[1;32m    185\u001B[0m     logger\u001B[38;5;241m.\u001B[39minfo(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFunction task started: `\u001B[39m\u001B[38;5;132;01m%s\u001B[39;00m\u001B[38;5;124m`\u001B[39m\u001B[38;5;124m\"\u001B[39m, running_task\u001B[38;5;241m.\u001B[39mexecutable\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__name__\u001B[39m)\n",
      "File \u001B[0;32m~/cognee/cognee/modules/pipelines/operations/run_tasks.py:155\u001B[0m, in \u001B[0;36mrun_tasks_base\u001B[0;34m(tasks, data, user)\u001B[0m\n\u001B[1;32m    147\u001B[0m send_telemetry(\n\u001B[1;32m    148\u001B[0m     \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mCoroutine Task Started\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    149\u001B[0m     user_id\u001B[38;5;241m=\u001B[39muser\u001B[38;5;241m.\u001B[39mid,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    152\u001B[0m     },\n\u001B[1;32m    153\u001B[0m )\n\u001B[1;32m    154\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m--> 155\u001B[0m     task_result \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m running_task\u001B[38;5;241m.\u001B[39mrun(\u001B[38;5;241m*\u001B[39margs)\n\u001B[1;32m    157\u001B[0m     \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mfor\u001B[39;00m result \u001B[38;5;129;01min\u001B[39;00m run_tasks_base(leftover_tasks, task_result, user):\n\u001B[1;32m    158\u001B[0m         \u001B[38;5;28;01myield\u001B[39;00m result\n",
      "File \u001B[0;32m~/cognee/cognee/tasks/graph/extract_graph_from_data.py:58\u001B[0m, in \u001B[0;36mextract_graph_from_data\u001B[0;34m(data_chunks, graph_model)\u001B[0m\n\u001B[1;32m     54\u001B[0m \u001B[38;5;250m\u001B[39m\u001B[38;5;124;03m\"\"\"Extracts and integrates a knowledge graph from the text content of document chunks using a specified graph model.\"\"\"\u001B[39;00m\n\u001B[1;32m     55\u001B[0m chunk_graphs \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m asyncio\u001B[38;5;241m.\u001B[39mgather(\n\u001B[1;32m     56\u001B[0m     \u001B[38;5;241m*\u001B[39m[extract_content_graph(chunk\u001B[38;5;241m.\u001B[39mtext, graph_model) \u001B[38;5;28;01mfor\u001B[39;00m chunk \u001B[38;5;129;01min\u001B[39;00m data_chunks]\n\u001B[1;32m     57\u001B[0m )\n\u001B[0;32m---> 58\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;01mawait\u001B[39;00m integrate_chunk_graphs(data_chunks, chunk_graphs, graph_model)\n",
      "File \u001B[0;32m~/cognee/cognee/tasks/graph/extract_graph_from_data.py:43\u001B[0m, in \u001B[0;36mintegrate_chunk_graphs\u001B[0;34m(data_chunks, chunk_graphs, graph_model)\u001B[0m\n\u001B[1;32m     36\u001B[0m graph_nodes, graph_edges \u001B[38;5;241m=\u001B[39m expand_with_nodes_and_edges(\n\u001B[1;32m     37\u001B[0m     data_chunks,\n\u001B[1;32m     38\u001B[0m     chunk_graphs,\n\u001B[1;32m     39\u001B[0m     existing_edges_map,\n\u001B[1;32m     40\u001B[0m )\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(graph_nodes) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[0;32m---> 43\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m add_data_points(graph_nodes)\n\u001B[1;32m     45\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28mlen\u001B[39m(graph_edges) \u001B[38;5;241m>\u001B[39m \u001B[38;5;241m0\u001B[39m:\n\u001B[1;32m     46\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m graph_engine\u001B[38;5;241m.\u001B[39madd_edges(graph_edges)\n",
      "File \u001B[0;32m~/cognee/cognee/tasks/storage/add_data_points.py:37\u001B[0m, in \u001B[0;36madd_data_points\u001B[0;34m(data_points)\u001B[0m\n\u001B[1;32m     33\u001B[0m nodes, edges \u001B[38;5;241m=\u001B[39m deduplicate_nodes_and_edges(nodes, edges)\n\u001B[1;32m     35\u001B[0m graph_engine \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m get_graph_engine()\n\u001B[0;32m---> 37\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m index_data_points(nodes)\n\u001B[1;32m     39\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m graph_engine\u001B[38;5;241m.\u001B[39madd_nodes(nodes)\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m graph_engine\u001B[38;5;241m.\u001B[39madd_edges(edges)\n",
      "File \u001B[0;32m~/cognee/cognee/tasks/storage/index_data_points.py:41\u001B[0m, in \u001B[0;36mindex_data_points\u001B[0;34m(data_points)\u001B[0m\n\u001B[1;32m     39\u001B[0m field_name \u001B[38;5;241m=\u001B[39m index_name_and_field[first_occurence \u001B[38;5;241m+\u001B[39m \u001B[38;5;241m1\u001B[39m :]\n\u001B[1;32m     40\u001B[0m \u001B[38;5;28;01mtry\u001B[39;00m:\n\u001B[0;32m---> 41\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m vector_engine\u001B[38;5;241m.\u001B[39mindex_data_points(index_name, field_name, indexable_points)\n\u001B[1;32m     42\u001B[0m \u001B[38;5;28;01mexcept\u001B[39;00m EmbeddingException \u001B[38;5;28;01mas\u001B[39;00m e:\n\u001B[1;32m     43\u001B[0m     logger\u001B[38;5;241m.\u001B[39mwarning(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mFailed to index data points for \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mindex_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m.\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mfield_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00me\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "File \u001B[0;32m~/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py:250\u001B[0m, in \u001B[0;36mLanceDBAdapter.index_data_points\u001B[0;34m(self, index_name, index_property_name, data_points)\u001B[0m\n\u001B[1;32m    247\u001B[0m \u001B[38;5;28;01masync\u001B[39;00m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21mindex_data_points\u001B[39m(\n\u001B[1;32m    248\u001B[0m     \u001B[38;5;28mself\u001B[39m, index_name: \u001B[38;5;28mstr\u001B[39m, index_property_name: \u001B[38;5;28mstr\u001B[39m, data_points: \u001B[38;5;28mlist\u001B[39m[DataPoint]\n\u001B[1;32m    249\u001B[0m ):\n\u001B[0;32m--> 250\u001B[0m     \u001B[38;5;28;01mawait\u001B[39;00m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mcreate_data_points(\n\u001B[1;32m    251\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mindex_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m_\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mindex_property_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m,\n\u001B[1;32m    252\u001B[0m         [\n\u001B[1;32m    253\u001B[0m             IndexSchema(\n\u001B[1;32m    254\u001B[0m                 \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(data_point\u001B[38;5;241m.\u001B[39mid),\n\u001B[1;32m    255\u001B[0m                 text\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mgetattr\u001B[39m(data_point, data_point\u001B[38;5;241m.\u001B[39mmetadata[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mindex_fields\u001B[39m\u001B[38;5;124m\"\u001B[39m][\u001B[38;5;241m0\u001B[39m]),\n\u001B[1;32m    256\u001B[0m             )\n\u001B[1;32m    257\u001B[0m             \u001B[38;5;28;01mfor\u001B[39;00m data_point \u001B[38;5;129;01min\u001B[39;00m data_points\n\u001B[1;32m    258\u001B[0m         ],\n\u001B[1;32m    259\u001B[0m     )\n",
      "File \u001B[0;32m~/cognee/cognee/infrastructure/databases/vector/lancedb/LanceDBAdapter.py:118\u001B[0m, in \u001B[0;36mLanceDBAdapter.create_data_points\u001B[0;34m(self, collection_name, data_points)\u001B[0m\n\u001B[1;32m    107\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m LanceDataPoint[\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mget_data_point_schema(\u001B[38;5;28mtype\u001B[39m(data_point))](\n\u001B[1;32m    108\u001B[0m         \u001B[38;5;28mid\u001B[39m\u001B[38;5;241m=\u001B[39m\u001B[38;5;28mstr\u001B[39m(data_point\u001B[38;5;241m.\u001B[39mid),\n\u001B[1;32m    109\u001B[0m         vector\u001B[38;5;241m=\u001B[39mvector,\n\u001B[1;32m    110\u001B[0m         payload\u001B[38;5;241m=\u001B[39mproperties,\n\u001B[1;32m    111\u001B[0m     )\n\u001B[1;32m    113\u001B[0m lance_data_points \u001B[38;5;241m=\u001B[39m [\n\u001B[1;32m    114\u001B[0m     create_lance_data_point(data_point, data_vectors[data_point_index])\n\u001B[1;32m    115\u001B[0m     \u001B[38;5;28;01mfor\u001B[39;00m (data_point_index, data_point) \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28menumerate\u001B[39m(data_points)\n\u001B[1;32m    116\u001B[0m ]\n\u001B[0;32m--> 118\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m (\n\u001B[1;32m    119\u001B[0m     collection\u001B[38;5;241m.\u001B[39mmerge_insert(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mid\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m    120\u001B[0m     \u001B[38;5;241m.\u001B[39mwhen_matched_update_all()\n\u001B[1;32m    121\u001B[0m     \u001B[38;5;241m.\u001B[39mwhen_not_matched_insert_all()\n\u001B[1;32m    122\u001B[0m     \u001B[38;5;241m.\u001B[39mexecute(lance_data_points)\n\u001B[1;32m    123\u001B[0m )\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py:2765\u001B[0m, in \u001B[0;36mAsyncTable._do_merge\u001B[0;34m(self, merge, new_data, on_bad_vectors, fill_value)\u001B[0m\n\u001B[1;32m   2763\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m fill_value \u001B[38;5;129;01mis\u001B[39;00m \u001B[38;5;28;01mNone\u001B[39;00m:\n\u001B[1;32m   2764\u001B[0m     fill_value \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m\n\u001B[0;32m-> 2765\u001B[0m data, _ \u001B[38;5;241m=\u001B[39m \u001B[43m_sanitize_data\u001B[49m\u001B[43m(\u001B[49m\n\u001B[1;32m   2766\u001B[0m \u001B[43m    \u001B[49m\u001B[43mnew_data\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2767\u001B[0m \u001B[43m    \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2768\u001B[0m \u001B[43m    \u001B[49m\u001B[43mmetadata\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mmetadata\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2769\u001B[0m \u001B[43m    \u001B[49m\u001B[43mon_bad_vectors\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mon_bad_vectors\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2770\u001B[0m \u001B[43m    \u001B[49m\u001B[43mfill_value\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mfill_value\u001B[49m\u001B[43m,\u001B[49m\n\u001B[1;32m   2771\u001B[0m \u001B[43m\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m   2772\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data, pa\u001B[38;5;241m.\u001B[39mTable):\n\u001B[1;32m   2773\u001B[0m     data \u001B[38;5;241m=\u001B[39m pa\u001B[38;5;241m.\u001B[39mRecordBatchReader\u001B[38;5;241m.\u001B[39mfrom_batches(data\u001B[38;5;241m.\u001B[39mschema, data\u001B[38;5;241m.\u001B[39mto_batches())\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py:164\u001B[0m, in \u001B[0;36m_sanitize_data\u001B[0;34m(data, schema, metadata, on_bad_vectors, fill_value)\u001B[0m\n\u001B[1;32m    157\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[38;5;21m_sanitize_data\u001B[39m(\n\u001B[1;32m    158\u001B[0m     data: Any,\n\u001B[1;32m    159\u001B[0m     schema: Optional[pa\u001B[38;5;241m.\u001B[39mSchema] \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m   (...)\u001B[0m\n\u001B[1;32m    162\u001B[0m     fill_value: \u001B[38;5;28mfloat\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;241m0.0\u001B[39m,\n\u001B[1;32m    163\u001B[0m ):\n\u001B[0;32m--> 164\u001B[0m     data \u001B[38;5;241m=\u001B[39m \u001B[43m_coerce_to_table\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    166\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m metadata:\n\u001B[1;32m    167\u001B[0m         data \u001B[38;5;241m=\u001B[39m _append_vector_col(data, metadata, schema)\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/lancedb/table.py:117\u001B[0m, in \u001B[0;36m_coerce_to_table\u001B[0;34m(data, schema)\u001B[0m\n\u001B[1;32m    115\u001B[0m         schema \u001B[38;5;241m=\u001B[39m data[\u001B[38;5;241m0\u001B[39m]\u001B[38;5;241m.\u001B[39m\u001B[38;5;18m__class__\u001B[39m\u001B[38;5;241m.\u001B[39mto_arrow_schema()\n\u001B[1;32m    116\u001B[0m     data \u001B[38;5;241m=\u001B[39m [model_to_dict(d) \u001B[38;5;28;01mfor\u001B[39;00m d \u001B[38;5;129;01min\u001B[39;00m data]\n\u001B[0;32m--> 117\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mpa\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mTable\u001B[49m\u001B[38;5;241;43m.\u001B[39;49m\u001B[43mfrom_pylist\u001B[49m\u001B[43m(\u001B[49m\u001B[43mdata\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mschema\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[43mschema\u001B[49m\u001B[43m)\u001B[49m\n\u001B[1;32m    118\u001B[0m \u001B[38;5;28;01melif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(data[\u001B[38;5;241m0\u001B[39m], pa\u001B[38;5;241m.\u001B[39mRecordBatch):\n\u001B[1;32m    119\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m pa\u001B[38;5;241m.\u001B[39mTable\u001B[38;5;241m.\u001B[39mfrom_batches(data, schema\u001B[38;5;241m=\u001B[39mschema)\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/pyarrow/table.pxi:2032\u001B[0m, in \u001B[0;36mpyarrow.lib._Tabular.from_pylist\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/pyarrow/table.pxi:6396\u001B[0m, in \u001B[0;36mpyarrow.lib._from_pylist\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/pyarrow/table.pxi:4851\u001B[0m, in \u001B[0;36mpyarrow.lib.Table.from_arrays\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/pyarrow/table.pxi:1608\u001B[0m, in \u001B[0;36mpyarrow.lib._sanitize_arrays\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/pyarrow/array.pxi:402\u001B[0m, in \u001B[0;36mpyarrow.lib.asarray\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/pyarrow/array.pxi:372\u001B[0m, in \u001B[0;36mpyarrow.lib.array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/pyarrow/array.pxi:42\u001B[0m, in \u001B[0;36mpyarrow.lib._sequence_to_array\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:155\u001B[0m, in \u001B[0;36mpyarrow.lib.pyarrow_internal_check_status\u001B[0;34m()\u001B[0m\n",
      "File \u001B[0;32m~/cognee/.venv/lib/python3.11/site-packages/pyarrow/error.pxi:92\u001B[0m, in \u001B[0;36mpyarrow.lib.check_status\u001B[0;34m()\u001B[0m\n",
      "\u001B[0;31mArrowInvalid\u001B[0m: Length of item not correct: expected 768 but got array of size 3072"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "print(\"\\n==== ONTOLOGY-ENHANCED KNOWLEDGE GRAPH SEARCH RESULTS ====\")\n",
    "# Now, search using the ontology-enhanced graph\n",
    "await prune_data(keep_dataset=True)  # Keep dataset but remove processing results\n",
    "await process_with_ontology()\n",
    "\n",
    "for query in queries:\n",
    "    print(f\"\\nQuery: {query}\")\n",
    "    results = await search(query_type=SearchType.GRAPH_COMPLETION, query_text=query)\n",
    "    print(\"Results:\")\n",
    "    for i, result in enumerate(results[:3]):\n",
    "        print(f\"{i+1}. {result}\")"
   ],
   "id": "b58259d4a321d4b7"
  },
  {
   "cell_type": "code",
   "id": "3bbb2603",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T06:13:59.467229Z",
     "start_time": "2025-02-26T06:13:59.462646Z"
    }
   },
   "source": "",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "e24404e0",
   "metadata": {},
   "source": [
    "\n",
    "## 6. Key Differences and Benefits\n",
    "\n",
    "### Without Ontology:\n",
    "- **Knowledge is limited to extracted information**: Only relationships and entities explicitly mentioned in the text are captured\n",
    "- **No hierarchical understanding**: Lacks class/subclass relationships unless explicitly stated\n",
    "- **Missing implicit connections**: Cannot infer relationships that weren't explicitly stated\n",
    "- **Domain knowledge is limited**: No external domain knowledge beyond the processed content\n",
    "\n",
    "### With Ontology:\n",
    "- **Enhanced semantic understanding**: Integration with domain ontologies provides richer semantic context\n",
    "- **Hierarchical relationships**: Class/subclass relationships from the ontology enrich the graph\n",
    "- **Inference capabilities**: Can infer relationships based on ontological axioms\n",
    "- **Domain knowledge enrichment**: External knowledge from the ontology supplements extracted information\n",
    "- **Standardized terminology**: Entities are mapped to standardized ontology concepts\n",
    "- **Better query answering**: More comprehensive answers due to extended knowledge\n",
    "\n",
    "## 7. Visualizations and Metrics\n",
    "\n",
    "Here are some key metrics to observe in the visualizations:\n",
    "\n",
    "1. **Node count**: The ontology-enhanced graph typically has more nodes\n",
    "2. **Edge density**: More connections between nodes in the ontology version\n",
    "3. **Clustering coefficient**: Often higher in the ontology version due to richer relationships\n",
    "4. **Average path length**: May be shorter in the ontology version due to additional connections\n",
    "5. **Connected components**: The ontology version usually has fewer isolated subgraphs\n",
    "\n",
    "## 8. Running the Demo\n",
    "\n",
    "Execute the following to run the complete demo:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "id": "69292dd6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T06:14:50.936865Z",
     "start_time": "2025-02-26T06:14:07.449868Z"
    }
   },
   "source": [
    "\n",
    "\n",
    "print(\"Starting Ontology Comparison Demo\")\n",
    "await prepare_data()\n",
    "\n",
    "print(\"\\nProcessing without ontology...\")\n",
    "standard_graph = await process_without_ontology()\n",
    "\n",
    "print(\"\\nProcessing with ontology...\")\n",
    "ontology_graph = await process_with_ontology()\n",
    "\n",
    "print(\"\\nComparing search results...\")\n",
    "await compare_search_results()\n",
    "\n",
    "print(\"\\nDemo completed!\")\n",
    "print(f\"Standard graph visualization: {standard_graph}\")\n",
    "print(f\"Ontology graph visualization: {ontology_graph}\")\n",
    "\n",
    "\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting Ontology Comparison Demo\n",
      "2025-02-25 22:14:07,451 - INFO - Graph deleted successfully.\n",
      "2025-02-25 22:14:07,456 - INFO - Database deleted successfully.\n",
      "User d4017493-773a-447e-b66f-9b186b6e4301 has registered.\n",
      "<cognee.modules.pipelines.models.PipelineRun.PipelineRun object at 0x121a69390>\n",
      "2025-02-25 22:14:07,532 - INFO - Pipeline run started: `4b84e400-23fc-5976-bbb4-f8ee303eed81`\n",
      "2025-02-25 22:14:07,533 - INFO - Coroutine task started: `resolve_data_directories`\n",
      "2025-02-25 22:14:07,533 - INFO - Coroutine task started: `ingest_data`\n",
      "2025-02-25 22:14:07,761 - INFO - Coroutine task completed: `ingest_data`\n",
      "2025-02-25 22:14:07,761 - INFO - Coroutine task completed: `resolve_data_directories`\n",
      "2025-02-25 22:14:07,761 - INFO - Pipeline run completed: `4b84e400-23fc-5976-bbb4-f8ee303eed81`\n",
      "<cognee.modules.pipelines.models.PipelineRun.PipelineRun object at 0x121780650>\n",
      "Data prepared successfully\n",
      "\n",
      "Processing without ontology...\n",
      "<cognee.modules.pipelines.models.PipelineRun.PipelineRun object at 0x121fb3550>\n",
      "2025-02-25 22:14:07,777 - INFO - Pipeline run started: `ea9bd1d8-7bd9-5908-a88b-1192a13ed265`\n",
      "2025-02-25 22:14:07,777 - INFO - Coroutine task started: `classify_documents`\n",
      "2025-02-25 22:14:07,778 - INFO - Coroutine task started: `check_permissions_on_documents`\n",
      "2025-02-25 22:14:07,780 - INFO - Async generator task started: `extract_chunks_from_documents`\n",
      "2025-02-25 22:14:19,660 - INFO - Coroutine task started: `extract_graph_from_data`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[92m22:14:19 - LiteLLM:INFO\u001B[0m: utils.py:2784 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 22:14:19,678 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[92m22:14:19 - LiteLLM:INFO\u001B[0m: utils.py:2784 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 22:14:19,681 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-25 22:14:23,469 - WARNING - File /Users/vasilije/cognee/cognee/.cognee_system/databases/cognee_graph.pkl not found. Initializing an empty graph.\n",
      "2025-02-25 22:14:24,787 - INFO - Coroutine task started: `add_data_points`\n",
      "2025-02-25 22:14:26,201 - INFO - Coroutine task completed: `add_data_points`\n",
      "2025-02-25 22:14:26,201 - INFO - Coroutine task completed: `extract_graph_from_data`\n",
      "2025-02-25 22:14:26,202 - INFO - Async generator task completed: `extract_chunks_from_documents`\n",
      "2025-02-25 22:14:26,202 - INFO - Coroutine task completed: `check_permissions_on_documents`\n",
      "2025-02-25 22:14:26,202 - INFO - Coroutine task completed: `classify_documents`\n",
      "2025-02-25 22:14:26,202 - INFO - Pipeline run completed: `ea9bd1d8-7bd9-5908-a88b-1192a13ed265`\n",
      "<cognee.modules.pipelines.models.PipelineRun.PipelineRun object at 0x121f21c90>\n",
      "2025-02-25 22:14:26,211 - INFO - Graph visualization saved as /Users/vasilije/cognee/cognee/ontology_testing_SANDBOX/Ontology_demo/.artifacts/standard_graph_visualization.html\n",
      "2025-02-25 22:14:26,211 - INFO - The HTML file has been stored at path: /Users/vasilije/cognee/cognee/ontology_testing_SANDBOX/Ontology_demo/.artifacts/standard_graph_visualization.html\n",
      "Standard graph saved to: /Users/vasilije/cognee/cognee/ontology_testing_SANDBOX/Ontology_demo/.artifacts/standard_graph_visualization.html\n",
      "\n",
      "Processing with ontology...\n",
      "Ontology loaded successfully.\n",
      "<cognee.modules.pipelines.models.PipelineRun.PipelineRun object at 0x1222d4e10>\n",
      "2025-02-25 22:14:26,230 - INFO - Pipeline run started: `af81ab41-8243-522f-a10a-b7b5febcc577`\n",
      "2025-02-25 22:14:26,230 - INFO - Coroutine task started: `classify_documents`\n",
      "2025-02-25 22:14:26,230 - INFO - Coroutine task started: `check_permissions_on_documents`\n",
      "2025-02-25 22:14:26,233 - INFO - Async generator task started: `extract_chunks_from_documents`\n",
      "2025-02-25 22:14:37,859 - INFO - Coroutine task started: `owl_ontology_merging_layer`\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[92m22:14:37 - LiteLLM:INFO\u001B[0m: utils.py:2784 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 22:14:37,864 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[92m22:14:37 - LiteLLM:INFO\u001B[0m: utils.py:2784 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 22:14:37,866 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "2025-02-25 22:14:49,526 - INFO - Coroutine task completed: `owl_ontology_merging_layer`\n",
      "2025-02-25 22:14:49,526 - INFO - Async generator task completed: `extract_chunks_from_documents`\n",
      "2025-02-25 22:14:49,526 - INFO - Coroutine task completed: `check_permissions_on_documents`\n",
      "2025-02-25 22:14:49,526 - INFO - Coroutine task completed: `classify_documents`\n",
      "2025-02-25 22:14:49,527 - INFO - Pipeline run completed: `af81ab41-8243-522f-a10a-b7b5febcc577`\n",
      "<cognee.modules.pipelines.models.PipelineRun.PipelineRun object at 0x1228a0d50>\n",
      "The query is What are the exact cars produced by Audi and what are their types?:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[92m22:14:50 - LiteLLM:INFO\u001B[0m: utils.py:2784 - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-02-25 22:14:50,329 - INFO - \n",
      "LiteLLM completion() model= gemini-1.5-flash; provider = gemini\n",
      "{\"Audi_cars\": [\"Audi_etron\", \"Audi_r8\", \"Audi_a8\"]}\n",
      "2025-02-25 22:14:50,914 - INFO - Graph visualization saved as /Users/vasilije/cognee/cognee/ontology_testing_SANDBOX/Ontology_demo/.artifacts/ontology_graph_visualization.html\n",
      "2025-02-25 22:14:50,915 - INFO - The HTML file has been stored at path: /Users/vasilije/cognee/cognee/ontology_testing_SANDBOX/Ontology_demo/.artifacts/ontology_graph_visualization.html\n",
      "Ontology-enhanced graph saved to: /Users/vasilije/cognee/cognee/ontology_testing_SANDBOX/Ontology_demo/.artifacts/ontology_graph_visualization.html\n",
      "\n",
      "Comparing search results...\n",
      "==== STANDARD KNOWLEDGE GRAPH SEARCH RESULTS ====\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "prune_data() got an unexpected keyword argument 'keep_dataset'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[0;32mIn[22], line 11\u001B[0m\n\u001B[1;32m      8\u001B[0m ontology_graph \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mawait\u001B[39;00m process_with_ontology()\n\u001B[1;32m     10\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mComparing search results...\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m compare_search_results()\n\u001B[1;32m     13\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;130;01m\\n\u001B[39;00m\u001B[38;5;124mDemo completed!\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mStandard graph visualization: \u001B[39m\u001B[38;5;132;01m{\u001B[39;00mstandard_graph\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m\"\u001B[39m)\n",
      "Cell \u001B[0;32mIn[21], line 11\u001B[0m, in \u001B[0;36mcompare_search_results\u001B[0;34m()\u001B[0m\n\u001B[1;32m      9\u001B[0m \u001B[38;5;28mprint\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124m==== STANDARD KNOWLEDGE GRAPH SEARCH RESULTS ====\u001B[39m\u001B[38;5;124m\"\u001B[39m)\n\u001B[1;32m     10\u001B[0m \u001B[38;5;66;03m# First, search using the standard graph\u001B[39;00m\n\u001B[0;32m---> 11\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m \u001B[43mprune_data\u001B[49m\u001B[43m(\u001B[49m\u001B[43mkeep_dataset\u001B[49m\u001B[38;5;241;43m=\u001B[39;49m\u001B[38;5;28;43;01mTrue\u001B[39;49;00m\u001B[43m)\u001B[49m  \u001B[38;5;66;03m# Keep dataset but remove processing results\u001B[39;00m\n\u001B[1;32m     12\u001B[0m \u001B[38;5;28;01mawait\u001B[39;00m process_without_ontology()\n\u001B[1;32m     14\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m query \u001B[38;5;129;01min\u001B[39;00m queries:\n",
      "\u001B[0;31mTypeError\u001B[0m: prune_data() got an unexpected keyword argument 'keep_dataset'"
     ]
    }
   ],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cad9819200293b7e",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T06:14:50.939123Z",
     "start_time": "2025-02-26T05:30:28.184445Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:httpx:HTTP Request: GET https://raw.githubusercontent.com/BerriAI/litellm/main/model_prices_and_context_window.json \"HTTP/1.1 200 OK\"/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/pydantic/_internal/_config.py:345: UserWarning: Valid config keys have changed in V2:\n",
      "* 'fields' has been removed\n",
      "  warnings.warn(message, UserWarning)\n",
      "/Users/vasilije/cognee/.venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "INFO:cognee.modules.visualization.cognee_network_visualization:Graph visualization saved as /Users/vasilije/cognee/cognee/ontology_testing_SANDBOX/Ontology_demo/.artifacts/graph_visualization.htmlINFO:root:The HTML file has been stored at path: /Users/vasilije/cognee/cognee/ontology_testing_SANDBOX/Ontology_demo/.artifacts/graph_visualization.html"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90ff3c07ac4b13d6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T05:09:05.027576Z",
     "start_time": "2025-02-26T05:09:05.024134Z"
    }
   },
   "outputs": [],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cc439d80dfdacae",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-26T05:09:09.310652Z",
     "start_time": "2025-02-26T05:09:09.308683Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/vasilije/cognee/cognee/ontology_testing_SANDBOX/Ontology_demo\n"
     ]
    }
   ],
   "source": ""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad409b4264dcc4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cognee-3fVNbOG1-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
